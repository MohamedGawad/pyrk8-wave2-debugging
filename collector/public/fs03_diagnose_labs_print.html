<p>This lab is intended to get the team familar with creating a pod in the team namespace.  There is no diagnosis or problem to be researched.  The lab is complete once the pod is created in the team namespace.</p><h4 id="resources">Resources</h4><ul><li>K8 yaml - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/house.yaml" target="_blank">house.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/house_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/house:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Docker</td><td align="left">CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;./house.sh&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Download the resource K8 yaml file.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit and save the file after replacing all references of <strong>&#60;team&#62;</strong> with your team name.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Create the K8 objects using oc create</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Did the pod deploy successfully?  If not, correct the issue and re-create the K8 objects.</td></tr></tbody></table><hr><p>To create the pod use the command: <strong>oc create -f &#60;file&#62;;</strong>   (replace &#60;file&#62; with the name of the yaml file you have saved an editied.)</p><hr><h4 id="diagnosis">Diagnosis</h4><p>No diagnosis is necessary for this lab.  A new pod should be created after editing the yaml file and using the oc create command.</p><h4 id="problem-discovered">Problem discovered</h4><p>N/A</p><h4 id="resolution">Resolution</h4><p>Edit the yaml file and modify all references of &#60;team&#62; to your team name.</p><br><pre><code>Example yaml file that needs to be edited.<br><br>--- # Fast Start :: Problem Diagnosis and Troubleshooting Lab <br>---<br>apiVersion: apps/v1    <br>kind: Deployment<br>metadata:<br>  name: &lt;team&gt;-house<br>  namespace: &lt;team&gt;<br>  labels:<br>    app: &lt;team&gt;-house<br>spec:<br>  selector:<br>    matchLabels:<br>      app: &lt;team&gt;-house<br>  replicas: 1<br>  template:<br>    metadata:<br>      labels:<br>        app: &lt;team&gt;-house<br>    spec:<br>      containers:<br>      - name: &lt;team&gt;-house<br>        image: ibmicpcoc/house:latest<br>        imagePullPolicy: Always<br>        command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;/app/avail.sh&quot;]<br>        env:<br>          - name: APP_NAMESPACE<br>            valueFrom:<br>              fieldRef:<br>                fieldPath: metadata.namespace<br>          - name: APP_NAME<br>            valueFrom:<br>              fieldRef:<br>                fieldPath: metadata.name<br>          - name: COLLECTOR_CONFIG<br>            valueFrom: <br>              configMapKeyRef:<br>                name: &lt;team&gt;-collector-config<br>                key: COLLECTOR_CONFIG<br>          - name: INSTRUCTOR_CONFIG<br>            valueFrom: <br>              configMapKeyRef:<br>                name: &lt;team&gt;-collector-config<br>                key: INSTRUCTOR_CONFIG<br>        resources:<br>          requests:<br>            cpu: 100m<br>            memory: 100Mi</code></pre><br><p>Saved the modified file and create the pod &quot;house&quot;.</p><br><br><pre><code>Command to create the K8 objects:<br>    oc create -f house.yaml<br><br>Result output:<br>    deployment.apps/house created<br><br><br>----<br>Verify the pod deployed successfully.<br><br>Command to get pods in namespace:<br>    oc -n &lt;team&gt; get pods         # change &lt;team&gt; to your team namespace</code></pre><br><hr><br><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><p>Use the debug flow to guide the steps you should attempt in diagnosis of the issue.</p><h4 id="resources-1">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/baker.yaml" target="_blank">baker.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/baker_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-1">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/baker:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Docker</td><td align="left">CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;./baker.sh&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Download the resource K8 yaml file.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit and save the file after replacing all references of &#60;team&#62; with your team name / namespace.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research why the pod did not deploy.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Resolve the issue and create the K8 objects.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Did the pod deploy successfully?  If not, correct the issue and re-create the K8 objects.</td></tr></tbody></table><hr><p>Deployment.spec.template.spec.containers expects an array of entires.  </p><p>Arrays are defined with a hyphen.</p><p>Review and compare the <strong>house.yaml</strong> file for an example of properly defined K8 objects.</p><hr><h4 id="diagnosis-1">Diagnosis</h4><p>When attempting to create the pod the yaml is not properly defined.  This error message is being shown:</p><p>error: error validating &quot;baker.yaml&quot;: error validating data: ValidationError(Deployment.spec.template.spec.containers): invalid type for io.k8s.api.core.v1.PodSpec.containers: got &quot;map&quot;, expected &quot;array&quot;; if you choose to ignore these errors, turn validation off with --validate=false</p><h4 id="problem-discovered-1">Problem discovered</h4><p>The Deployment.spec.template.spec.containers portion of the yaml file is not properly formatted.  Got &quot;map&quot;, expected &quot;array&quot;.  Container does not have an array of entires.</p><h4 id="resolution-1">Resolution</h4><p>Edit the yaml file and correct the definition to include a hyphen before the &quot;name:&quot; parameter of the containers portion.</p><br><pre><code>Example saved file with hyphen  (portion of file shown below)<br><br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: &lt;team&gt;-baker<br>  namespace: pink<br>  labels:<br>    app: &lt;team&gt;-baker<br>spec:<br>  selector:<br>    matchLabels:<br>      app: &lt;team&gt;-baker<br>  replicas: 1<br>  template:<br>    metadata:<br>      labels:<br>        app: &lt;team&gt;-baker<br>    spec:<br>      containers:<br>      - name: &lt;team&gt;-baker                 &lt;=== Add the hyphen to this line <br>        image: ibmicpcoc/baker:latest<br>        imagePullPolicy: Always</code></pre><br><p>Saved the modified file and create the pod &quot;baker&quot;.</p><br><br><pre><code>Command to create the K8 objects:<br>    oc create -f baker.yaml<br><br>Result output:<br>    deployment.apps/baker created<br><br><br>----<br>Verify the pod deployed successfully.<br><br>Command to get pods in namespace:<br>    oc -n &lt;team&gt; get pods         # change &lt;team&gt; to your team namespace</code></pre><br><hr><br><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><p>Use the debug flow to guide the steps you should attempt in diagnosis of the issue.</p><hr><hr><h4 id="useful-information-2">Useful information</h4><p>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/carbon.yaml" target="_blank">carbon.yaml</a><br>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/carbon_Dockerfile">Dockerfile</a>  </p><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">spec.template.spec.containers[*].resouces.request.cpu</td><td align="left">100m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.template.spec.containers[*].resouces.request.memory:</td><td align="left">100Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.template.spec.containers[*].image:</td><td align="left">ibmicpcoc/carbon:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.template.spec.containers[*].ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Docker CMD</td><td align="left">[&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;./carbon.sh&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Within your team namespace diagnose the pod that begins with <strong>&#60;team&#62; -carbon</strong></td></tr><tr style="background-color: #f8f8f8;"><td align="left">Use the label option -l app=&#60;team&#62;-carbon when getting the pod information.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Download the resource K8 yaml file.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit and save the file after replacing all references of &#60;team&#62; with your team name / namespace.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Create the K8 objects.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Did the pod deploy successfully?  If not, correct the issue and re-create the K8 objects.</td></tr></tbody></table><hr><ul><li><p>Describe the pod.</p></li><li><p>Get events from the namespace, oc get events -n <strong>&#60;team&#62;</strong></p></li><li><p>A single cpu is defined with 1000m. The container cpu resources should use <strong>1/10</strong> of a cpu.  </p></li><li><p>Editing a running pod is another method to change the pod.  Use the command KUBE_EDITOR=&quot;nano&quot; oc edit deployment/<strong>&#60;team&#62;-carbon</strong>  and edit the running pod.  Nano is the editor defined in the command.  Remove the KUBE_EDITOR parm to use the default editor on your machine.</p></li></ul><hr><h4 id="diagnosis-2">Diagnosis</h4><p>When attempting to deploy the pod the yaml file is not properly defined.</p><p>Check the Pod status</p><br><pre><code>Command:<br>    oc -n &lt;team&gt; get pods -l app=&lt;team&gt;-carbon.  # replace &lt;team&gt; <br><br>Example output:<br>    NAME                              READY     STATUS    RESTARTS   AGE<br>    pink-carbon-5c96bc649-tjnhb       0/1       Pending   0          2m</code></pre><br><p>Describe the pod</p><br><br><pre><code>Name:               pink-carbon-5c96bc649-tjnhb<br>Namespace:          pink<br>Priority:           0<br>PriorityClassName:  &lt;none&gt;<br>Node:               &lt;none&gt;<br>Labels:             app=pink-carbon<br>                    pod-template-hash=175267205<br>Annotations:        kubernetes.io/psp=ibm-privileged-psp<br>Status:             Pending<br>IP:<br>Controlled By:      ReplicaSet/pink-carbon-5c96bc649<br>Containers:<br>  pink-carbon:<br>    Image:      ibmicpcoc/carbon:latest<br>    Port:       &lt;none&gt;<br>    Host Port:  &lt;none&gt;<br>    Requests:<br>      cpu:     25<br>      memory:  100Mi<br>    Environment:<br>      APP_NAMESPACE:      pink (v1:metadata.namespace)<br>      APP_NAME:           pink-carbon-5c96bc649-tjnhb (v1:metadata.name)<br>      COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;pink-collector-config&#39;&gt;   Optional: false<br>      INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;pink-collector-config&#39;&gt;  Optional: false<br>    Mounts:<br>      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mq64m (ro)<br>Conditions:<br>  Type           Status<br>  PodScheduled   False<br>Volumes:<br>  default-token-mq64m:<br>    Type:        Secret (a volume populated by a Secret)<br>    SecretName:  default-token-mq64m<br>    Optional:    false<br>QoS Class:       Burstable<br>Node-Selectors:  &lt;none&gt;<br>Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>                 node.kubernetes.io/not-ready:NoExecute for 300s<br>                 node.kubernetes.io/unreachable:NoExecute for 300s<br>Events:<br>  Type     Reason            Age                 From               Message<br>  ----     ------            ----                ----               -------<br>  Warning  FailedScheduling  58s (x121 over 5m)  default-scheduler  0/4 nodes are available: 4 Insufficient cpu.<br>$</code></pre><p>In the &quot;Events&quot; section review the &quot;Message&quot; from the entry with &quot;Type&quot; Warning and &quot;Reason&quot; FailedScheduling<br></p><br><pre><code>...  0/4 nodes are available: 4 Insufficient cpu.<br>$</code></pre><br><p>Example of Get Events in namespace</p><br><pre><code>Command:<br>    oc -n &lt;team&gt; get events<br><br>Example output:<br><br>LAST SEEN   FIRST SEEN   COUNT     NAME                                            KIND         SUBOBJECT                      TYPE      REASON              SOURCE                    MESSAGE<br>7m          7m           1         pink-carbon.157be1efb7ad1a77                    Deployment                                  Normal    ScalingReplicaSet   deployment-controller     Scaled up replica set pink-carbon-5c96bc649 to 1<br>7m          7m           1         pink-carbon-5c96bc649.157be1efb85494ba          ReplicaSet                                  Normal    SuccessfulCreate    replicaset-controller     Created pod: pink-carbon-5c96bc649-tjnhb<br>2m          7m           121       pink-carbon-5c96bc649-tjnhb.157be1efb858b4b3    Pod                                         Warning   FailedScheduling    default-scheduler         0/4 nodes are available: 4 Insufficient cpu.</code></pre><br><h4 id="problem-discovered-2">Problem discovered</h4><p>Events output indicates the pod is FailedScheduling because there are not enough CPU resources available.</p><h4 id="resolution-2">Resolution</h4><p>At least two methods exist to correct the issue.  </p><blockquote><p>The first method is deleting the old pod, edit the yaml file, and re-create the pod.  </p></blockquote><p>This approach is later referred to as: delete-create-pod</p><p>Edit the yaml file and modify <i>cpu</i> to decrease the amount of cpu to 10% of a single CPU.</p><br>Delete the running pod<br><pre><code>Command to delete the existing pod: <br>    oc delete -f carbon.yaml<br><br>Result output:<br>    deployment.apps &quot;carbon&quot; deleted</code></pre><br>Edit file carbon.yaml (only a portion of file shown below)<br><pre><code>    spec:<br>      selector:<br>        matchLabels:<br>         app: &lt;team&gt;-carbon<br>     replicas: 1<br>     template:<br>       metadata:<br>         labels:<br>           app: &lt;team&gt;-carbon<br>       spec:<br>         containers:<br>         - name: &lt;team&gt;-carbon<br>           image: ibmicpcoc/carbon:latest<br>           resources:<br>             requests:<br>               cpu: 25000m                   &lt;=== change value to 100m<br>               memory: 100Mi</code></pre><br>Create the k8 deployment<br><pre><code>Command:<br>    oc create -f carbon.yaml<br><br>Result output:<br>deployment.apps/&lt;team&gt;-carbon created</code></pre><br><blockquote><p>The second method is editing the running pod.  Edit and save edit the file, and re-create the pod.  </p></blockquote><p>This approach is later referred as: edit-running-pod</p><p>Edit the running pod.  The kubernetes object content is available in the editor (shown below).  Note the content has both the spec: and status: sections. </p><p>Locate the line cpu: &quot;25&quot; and change the line to cpu: 100m (without quotes)</p><br><pre><code>Command to edit the running pod:<br>    KUBE_EDITOR=&quot;nano&quot; oc edit deployment/&lt;team&gt;-carbon     # replace &lt;team&gt; <br><br>Content shown when editor is open.  The pink-carbon deployment is being shown:<br><br># Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,<br># and an empty file will abort the edit. If an error occurs while saving this file will be<br># reopened with the relevant failures.<br>#<br>apiVersion: extensions/v1beta1<br>kind: Deployment<br>metadata:<br>  annotations:<br>    deployment.kubernetes.io/revision: &quot;1&quot;<br>  creationTimestamp: 2019-01-21T14:01:56Z<br>  generation: 1<br>  labels:<br>    app: pink-carbon<br>  name: pink-carbon<br>  namespace: pink<br>  resourceVersion: &quot;5834141&quot;<br>  selfLink: /apis/extensions/v1beta1/namespaces/pink/deployments/pink-carbon<br>  uid: 1d02fbe9-1d85-11e9-b012-06ed6a534df5<br>spec:<br>  progressDeadlineSeconds: 600<br>  replicas: 1<br>  revisionHistoryLimit: 10<br>  selector:<br>    matchLabels:<br>      app: pink-carbon<br>  strategy:<br>    rollingUpdate:<br>      maxSurge: 25%<br>      maxUnavailable: 25%<br>    type: RollingUpdate<br>  template:<br>    metadata:<br>      creationTimestamp: null<br>      labels:<br>        app: pink-carbon<br>    spec:<br>      containers:<br>      - env:<br>        - name: APP_NAMESPACE<br>          valueFrom:<br>            fieldRef:<br>              apiVersion: v1<br>              fieldPath: metadata.namespace<br>        - name: APP_NAME<br>          valueFrom:<br>            fieldRef:<br>              apiVersion: v1<br>              fieldPath: metadata.name<br>        - name: COLLECTOR_CONFIG<br>          valueFrom:<br>            configMapKeyRef:<br>              key: COLLECTOR_CONFIG<br>              name: pink-collector-config<br>        - name: INSTRUCTOR_CONFIG<br>          valueFrom:<br>            configMapKeyRef:<br>              key: INSTRUCTOR_CONFIG<br>              name: pink-collector-config<br>        image: ibmicpcoc/carbon:latest<br>        imagePullPolicy: Always<br>        name: pink-carbon<br>        resources:<br>          requests:<br>            cpu: &quot;25&quot;                         &lt;=== change value to 100m without quotes<br>            memory: 100Mi<br>        terminationMessagePath: /dev/termination-log<br>        terminationMessagePolicy: File<br>      dnsPolicy: ClusterFirst<br>      restartPolicy: Always<br>      schedulerName: default-scheduler<br>      securityContext: {}<br>      terminationGracePeriodSeconds: 30<br>status:<br>  conditions:<br>  - lastTransitionTime: 2019-01-21T14:01:56Z<br>    lastUpdateTime: 2019-01-21T14:01:56Z<br>    message: Deployment does not have minimum availability.<br>    reason: MinimumReplicasUnavailable<br>    status: &quot;False&quot;<br>    type: Available<br>  - lastTransitionTime: 2019-01-21T14:11:57Z<br>    lastUpdateTime: 2019-01-21T14:11:57Z<br>    message: ReplicaSet &quot;pink-carbon-5c96bc649&quot; has timed out progressing.<br>    reason: ProgressDeadlineExceeded<br>    status: &quot;False&quot;<br>    type: Progressing<br>  observedGeneration: 1<br>  replicas: 1    <br><br><br>NOTE: You must save the file for the changes to take effect.<br><br>Result output:<br>    deployment.extensions/pink-carbon edited</code></pre><br><p>Did this resolve the issue?</p><br><br><pre><code>Command to get pods in namespace: <br>    oc -n &lt;namespace&gt; get pods<br><br>Example output:<br>    NAME                              READY     STATUS    RESTARTS   AGE<br>    pink-carbon-7784b95958-pctl5      1/1       Running   0          2m</code></pre><br><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><p>Use the debug flow to guide the steps you should attempt in diagnosis of the issue.</p><h4 id="resources-2">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/doors.yaml" target="_blank">doors.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/doors_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-3">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/doors:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Docker</td><td align="left">CMD [&quot;node&quot;, &quot;app.js&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Within your team namespace diagnose the pod that begins with &#60;team&#62;-doors</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Use the label option -l app=&#60;team&#62;-doors when getting the pod status.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Download the resource K8 yaml file.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Use either of the delete-create-pod or edit-running-pod approaches to resolve the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Did the pod deploy successfully?  If not, correct the issue and re-create the K8 objects.</td></tr></tbody></table><hr><p>Check the &quot;tag&quot; of the image that is being pulled. </p><hr><h4 id="diagnosis-3">Diagnosis</h4><p>Pod status</p><br><pre><code>Command:<br>    oc -n &lt;your namespace&gt; get pods -l app=&lt;team&gt;-doors    (replace &lt;team&gt; )<br><br>Example output:<br>NAME                         READY     STATUS             RESTARTS   AGE<br>pink-doors-78b7f6598d-p8kvf   0/1       ImagePullBackOff   0          10m</code></pre><br><p>Describe the pod (complete output from command is shown)</p><br><pre><code>Name:               pink-doors-78b7f6598d-p8kvf<br>Namespace:          pink<br>Priority:           0<br>PriorityClassName:  &lt;none&gt;<br>Node:               10.186.56.85/10.186.56.85<br>Start Time:         Mon, 21 Jan 2019 10:18:18 -0600<br>Labels:             app=pink-doors<br>                    pod-template-hash=3349118043<br>. . .<br>        portions of output removed<br>. . .<br><br>Events:<br>  Type     Reason     Age                From                   Message<br>  ----     ------     ----               ----                   -------<br>  Normal   Scheduled  46s                default-scheduler      Successfully assigned pink/pink-doors-78b7f6598d-p8kvf to 10.186.56.85<br>  Normal   Pulling    28s (x2 over 43s)  kubelet, 10.186.56.85  pulling image &quot;ibmicpcoc/doors:last&quot;<br>  Warning  Failed     27s (x2 over 43s)  kubelet, 10.186.56.85  Failed to pull image &quot;ibmicpcoc/doors:last&quot;: rpc error: code = Unknown desc = Error response from daemon: manifest for ibmicpcoc/doors:last not found<br>  Warning  Failed     27s (x2 over 43s)  kubelet, 10.186.56.85  Error: ErrImagePull<br>  Normal   BackOff    12s (x3 over 42s)  kubelet, 10.186.56.85  Back-off pulling image &quot;ibmicpcoc/doors:last&quot;<br>  Warning  Failed     12s (x3 over 42s)  kubelet, 10.186.56.85  Error: ImagePullBackOff</code></pre><p>Multiple Warning messages are displayed in the Event setion.  Review all of the Warning messages.</p><p>In the &quot;Events&quot; section review the &quot;Message&quot; from the entry with &quot;Type&quot; Warning and &quot;Reason&quot; Failed<br></p><br><pre><code>...  Failed to pull image &quot;ibmicpcoc/doors:last&quot;: rpc error: code = Unknown desc = Error response from daemon: manifest for ibmicpcoc/doors:last not found<br><br>(output is from the first Failed message)</code></pre><br><h4 id="problem-discovered-3">Problem discovered</h4><p>The image cannot be located as indicated by the &quot;Failed to pull image&quot; message.  The image tag last on the container is incorrect.  The image tag should be latest.</p><br><h4 id="resolution-3">Resolution</h4><p>The edit-running-pod is shown in the following example to resolve the issue:</p><br><pre><code>Command to edit the running pod:<br>    KUBE_EDITOR=&quot;nano&quot; oc -n &lt;team&gt; edit deployment/&lt;team&gt;-doors<br><br><br>Example is from the pink namespace.  Modify the tag of the image to &quot;latest&quot;<br><br><br># Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,<br># and an empty file will abort the edit. If an error occurs while saving this file will be<br># reopened with the relevant failures.<br>#<br>apiVersion: extensions/v1beta1<br>kind: Deployment<br>metadata:<br>  annotations:<br>    deployment.kubernetes.io/revision: &quot;1&quot;<br>  creationTimestamp: 2019-01-21T16:18:18Z<br>  generation: 1<br>  labels:<br>    app: pink-doors<br>  name: pink-doors<br>  namespace: pink<br>  resourceVersion: &quot;5853628&quot;<br>  selfLink: /apis/extensions/v1beta1/namespaces/pink/deployments/pink-doors<br>  uid: 29914949-1d98-11e9-b012-06ed6a534df5<br>spec:<br>  progressDeadlineSeconds: 600<br>  replicas: 1<br>  revisionHistoryLimit: 10<br>  selector:<br>    matchLabels:<br>      app: pink-doors<br>  strategy:<br>    rollingUpdate:<br>      maxSurge: 25%<br>      maxUnavailable: 25%<br>    type: RollingUpdate<br>  template:<br>    metadata:<br>      creationTimestamp: null<br>      labels:<br>        app: pink-doors<br>    spec:<br>      containers:<br>      - env:<br>        - name: APP_NAMESPACE<br>          valueFrom:<br>            fieldRef:<br>              apiVersion: v1<br>              fieldPath: metadata.namespace<br>        - name: APP_NAME<br>          valueFrom:<br>            fieldRef:<br>              apiVersion: v1<br>              fieldPath: metadata.name<br>        - name: COLLECTOR_CONFIG<br>          valueFrom:<br>            configMapKeyRef:<br>              key: COLLECTOR_CONFIG<br>              name: pink-collector-config<br>        - name: INSTRUCTOR_CONFIG<br>          valueFrom:<br>            configMapKeyRef:<br>              key: INSTRUCTOR_CONFIG<br>              name: pink-collector-config<br>        image: ibmicpcoc/doors:last             &lt;=== change the :last to :latest<br>        imagePullPolicy: Always<br><br><br>Ensure you have saved the modified file.<br><br><br>Result output:<br>    deployment/pink-doors<br></code></pre><br><br><br><p>Validate the pod status is Running.</p><br><pre><code>Command:<br>    oc -n &lt;team&gt; get pods<br><br>Example output:<br>NAME                              READY     STATUS    RESTARTS   AGE<br>pink-doors-767f49c748-6gvcg       1/1       Running   0          1m</code></pre><br><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><p>Use the debug flow to guide the steps you should attempt in diagnosis of the issue.</p><h4 id="resources-3">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/avail.yaml" target="_blank">avail.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/avail_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-4">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/avail:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Run K8 spec</td><td align="left">command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;/app/avail.sh&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Within the &quot;avail&quot; namespace research the pod that begins with &quot;avail&quot;.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Why is the pod not deploying?</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Review K8 definitions for controlling privleges e.g. PSP, RoleBinding, Roles etc.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Download the resource K8 yaml file.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the file replacing &#60;team&#62; with your team name.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Create the K8 objects.</td></tr></tbody></table><hr><p>What rolebinding is defined for avail namespace?<br>What rolebinding is defined for &#60;team&#62; namespace?<br>Review the clusterroles for the cluster.<br>Reivew the pod security policies for the cluster.  </p><hr><h4 id="diagnosis-4">Diagnosis</h4><br><pre><code>Command to check pods in namespace:    <br><br>    oc -n avail get pods<br><br>Example output:<br><br>NAME                         READY     STATUS                       RESTARTS   AGE<br>avail-all-65b8448469-rqt5g   0/1       CreateContainerConfigError   0          1d<br><br>Command to describe the selected pod in the namespace: <br><br>    oc -n avail describe pod avail-all-65b8448469-rqt5g<br><br>Example output:<br><br>Name:               avail-all-65b8448469-rqt5g<br>Namespace:          avail<br>Priority:           0<br>PriorityClassName:  &lt;none&gt;<br>Node:               10.186.56.85/10.186.56.85<br>Start Time:         Sat, 19 Jan 2019 13:57:24 -0600<br>Labels:             app=avail-all<br>                    pod-template-hash=2164004025<br><br>    . . . &lt; portions of the describe output not shown&gt; . . .<br><br>Events:<br>      Type     Reason     Age                 From                   Message<br>      ----     ------     ----                ----                   -------<br>      Normal   Scheduled  28m                 default-scheduler      Successfully assigned avail/avail-698964bc87-5k8vf to 10.186.56.85<br>      Normal   Pulled     26m (x8 over 28m)   kubelet, 10.186.56.85  Successfully pulled image &quot;avail&quot;<br>      Warning  Failed     26m (x8 over 28m)   kubelet, 10.186.56.85  Error: container has runAsNonRoot and image will run as root</code></pre><p>In the &quot;Events&quot; section review the &quot;Message&quot; from the entry with &quot;Type&quot; Warning and &quot;Reason&quot; Failed</p><br><pre><code>...  Error: container has runAsNonRoot and image will run as root</code></pre><p>What rolebinding are defined for the <strong>avail</strong> namespace? </p><br><pre><code>Command to check rolebindings:<br>    oc get rolebinding -n avail<br><br>Example output:<br><br>No resources found.</code></pre><p>Compare rolebindings for your <strong>team</strong> namespace. </p><br><pre><code>Command to check rolebindings:<br>    oc get rolebinding -n &lt;team&gt;<br><br>Example output:<br><br>NAME                                     AGE       ROLE                                     USERS     GROUPS                        SERVICEACCOUNTS<br>ibm-privileged-clusterrole-rolebinding   16h       ClusterRole/ibm-privileged-clusterrole             system:serviceaccounts:aqua   </code></pre><p>Review the clusterrole definitions for the cluster.</p><br><pre><code>Command to view clusterrole<br>    k get clusterrole<br><br>Example output:<br><br>NAME                                                                   AGE<br>admin                                                                  17h<br>cluster-admin                                                          17h<br>edit                                                                   17h<br>extension                                                              17h<br>ibm-anyuid-clusterrole                                                 17h<br>ibm-anyuid-hostaccess-clusterrole                                      17h<br>ibm-anyuid-hostpath-clusterrole                                        17h<br>ibm-cert-manager-cert-manager                                          17h<br>ibm-privileged-clusterrole                                             17h<br>ibm-restricted-clusterrole                                             17h<br>icp-admin-aggregate                                                    17h<br>icp-edit-aggregate                                                     17h<br>icp-operate-aggregate                                                  17h<br>icp-view-aggregate                                                     17h<br><br>    . . . data truncated . . . <br></code></pre><br><p>Describe the clusterrole for ibm-privileged-clusterrole</p><br><br><pre><code>Command to describe:<br>    oc describe clusterrole ibm-privileged-clusterrole<br><br>Example output;<br><br>Name:         ibm-privileged-clusterrole<br>Labels:       &lt;none&gt;<br>Annotations:  oc.kubernetes.io/last-applied-configuration={&quot;apiVersion&quot;:&quot;rbac.authorization.k8s.io/v1&quot;,&quot;kind&quot;:&quot;ClusterRole&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;ibm-privileged-clusterrole&quot;,&quot;namespace&quot;:&quot;&quot;},&quot;rul...<br>PolicyRule:<br>  Resources                       Non-Resource URLs  Resource Names        Verbs<br>  ---------                       -----------------  --------------        -----<br>  podsecuritypolicies.extensions  []                 [ibm-privileged-psp]  [use]</code></pre><br><p>Review the Pod Security Policies. </p><br><br><pre><code>Command to view Pod Security Policy:<br><br>    oc get psp<br><br>Example output:<br><br>NAME                        PRIV      CAPS                                                                                                                  SELINUX    RUNASUSER          FSGROUP     SUPGROUP    READONLYROOTFS   VOLUMES<br>ibm-anyuid-hostaccess-psp   false     SETPCAP,AUDIT_WRITE,CHOWN,NET_RAW,DAC_OVERRIDE,FOWNER,FSETID,KILL,SETUID,SETGID,NET_BIND_SERVICE,SYS_CHROOT,SETFCAP   RunAsAny   RunAsAny           RunAsAny    RunAsAny    false            *<br>ibm-anyuid-hostpath-psp     false     SETPCAP,AUDIT_WRITE,CHOWN,NET_RAW,DAC_OVERRIDE,FOWNER,FSETID,KILL,SETUID,SETGID,NET_BIND_SERVICE,SYS_CHROOT,SETFCAP   RunAsAny   RunAsAny           RunAsAny    RunAsAny    false            *<br>ibm-anyuid-psp              false     SETPCAP,AUDIT_WRITE,CHOWN,NET_RAW,DAC_OVERRIDE,FOWNER,FSETID,KILL,SETUID,SETGID,NET_BIND_SERVICE,SYS_CHROOT,SETFCAP   RunAsAny   RunAsAny           RunAsAny    RunAsAny    false            configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim<br>ibm-privileged-psp          true      *                                                                                                                     RunAsAny   RunAsAny           RunAsAny    RunAsAny    false            *<br>ibm-restricted-psp          false                                                                                                                           RunAsAny   MustRunAsNonRoot   MustRunAs   MustRunAs   false            configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim<br><br></code></pre><br><h4 id="problem-discovered-4">Problem discovered</h4><p>The &quot;avail&quot; namespace does not have the proper authourity to run the &quot;avail&quot; pod.  The avail pod must be deployed within a namespace that has the proper authority.  Your team namespace has the proper authority.</p><h4 id="resolution-4">Resolution</h4><p>Download the K8 Yaml file from the resources section and save locally.  Once saved, edit the file and change the namespace metadata parameter in the file and deploy the pod.</p><br><pre><code>Example saved file avail.yaml (only a portion of file is shown below)<br><br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: avail<br>  namespace: &lt;team&gt;       # change &lt;team&gt; to your namesapce and save the file<br><br>----<br>Command to create the new pod:<br>    oc create -f avail.yaml<br><br>Result output:<br>    deployment.apps/avail created<br><br><br>----<br>Verify issue is resolved. Pod status should be &quot;Running&quot;:<br><br>Command to get pods in namespace:<br>    oc -n &lt;team&gt; get pods        # change &lt;team&gt; to your team namespace<br><br>Example output:<br>    avail-698964bc87-2fpw8   1/1       Running   0          1m</code></pre><br><hr><br><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-4">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/eagle.yaml" target="_blank">eagle.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/eagle_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-5">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/eagle:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">4100</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Docker</td><td align="left">CMD [&quot;node&quot;, &quot;server.js&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">This lab uses the pod with a name that starts with <strong>&#60;team&#62;-eagle</strong></td></tr><tr style="background-color: #f8f8f8;"><td align="left">The web application is not working properly. The application is has a K8 Deployment and Service defined.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research why the web application is not working properly.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Once you have resolved the issue locate the NodePort (is a number in the 30000 range) for the service.  Example: oc get svc -n &#60;team&#62; -o wide</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Using the same IP that has been used to access the Collector now access the the web application using the newly located node port number.  Example url to access web application:  <strong><a href="http://xxx.xxx.xxx.xxx:NodePort" target="_blank">http://xxx.xxx.xxx.xxx:NodePort</a></strong></td></tr><tr style="background-color: #f8f8f8;"><td align="left">Once the web application is successfully accessed press the button to complete the lab.</td></tr></tbody></table><hr><ul><li>All exposed port definitions must match.  </li><li>What port should the application be available on?  Refer to useful information.</li></ul><hr><h4 id="diagnosis-5">Diagnosis</h4><p>The pod is running successfully yet describing the pod can provide information about the configured K8 objects.  Describe the pod that begins with: &#60;team&#62;-eagle</p><br><pre><code>Commad to get pods in namespace<br>    oc -n &lt;team&gt; get pods               # Replace &lt;team&gt; with namespace name<br><br>Command to describe the pod                  # Use the pod name from the previous output<br>    oc -n &lt;team&gt; describe pod &lt;pod&gt;     # Replace &lt;team&gt; with namespace name </code></pre><p>Review the port definitions from the describe output</p><br><pre><code>Show something here</code></pre><h4 id="problem-discovered-5">Problem discovered</h4><p>The ports do not match for the Deployment and Service definitions.</p><h4 id="resolution-5">Resolution</h4><p>Edit the Service definition and change the port from 4010 to 4100.</p><br><pre><code>Add detailed steps here</code></pre><br><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-5">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/floor.yaml" target="_blank">floor.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/floor_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-6">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/floor:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A container wihtin a successfully deployed pod is not working properly.  Research the running container to diagnose the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">View the logs of the running container.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Correct the issue inside the running container.</td></tr></tbody></table><hr><ul><li>Exec into the running container</li><li>Use touch, nano, or echo with piping to assit in resolving the issue</li></ul><hr><h4 id="diagnosis-6">Diagnosis</h4><p>Check the logs of the running container that begins with &#60;team&#62;</p><br><pre><code>Command to get pods in namespace<br>    oc -n &lt;team&gt; get pods                 &lt;=== Replace &lt;team&gt;<br><br>Example output from &quot;pink&quot; namespace    <br>    NAME                              READY     STATUS    RESTARTS   AGE<br>    pink-floor-6ff9f54f44-zpchp       1/1       Running   0          41s<br><br><br>Get the logs for the pod                      <br>    oc -n &lt;team&gt; logs -f &lt;pod&gt;            &lt;=== Replace &lt;team&gt; and &lt;pod&gt;<br>                                               Use the pod name from the get pods result<br><br>Instructions from viewing the log<br><br>1/21/2019, 10:21:14 PM :: clnt012i - Check for file: /app/team.txt check count: 43<br>1/21/2019, 10:21:14 PM :: clnt013i - The file team.txt in the /app directory must exist for this lab to be completed.<br>1/21/2019, 10:21:14 PM :: clnt014i - Create the file in the running container.<br><br></code></pre><br><h4 id="problem-discovered-6">Problem discovered</h4><p>The file team.txt is missing from the /app directory in the running container.</p><h4 id="resolution-6">Resolution</h4><p>Two methods can be used to resolve of creating the file.  </p><blockquote><p>First method is to run a &quot;command&quot; using the oc CLI from outside the container.</p></blockquote><br><br><pre><code>Command to get pods in namespace<br>    oc -n &lt;team&gt; get pods                 &lt;=== Replace &lt;team&gt;<br><br>Example output from &quot;pink&quot; namespace    <br>    NAME                              READY     STATUS    RESTARTS   AGE<br>    pink-floor-6ff9f54f44-zpchp       1/1       Running   0          41s<br><br><br>Add the team.txt file using the touch command from outside the container.<br>    oc exec -n pink pink-floor-6ff9f54f44-zpchp -- sh -c &quot;touch /app/team.txt&quot;<br><br>    The above command is using &#39;sh&#39;.  The &#39;sh&#39; capability must be installed in the container for this to work.<br><br><br>Example result output:  (wait a few seconds for the messages to show)<br><br>1/21/2019, 10:25:30 PM :: clnt014i - Create the file in the running container.<br>1/21/2019, 10:25:45 PM :: ----------------------------------------------------------------------------------<br>1/21/2019, 10:25:45 PM :: clnt008i - File located.  Reporting to collector.<br>1/21/2019, 10:25:45 PM :: ----------------------------------------------------------------------------------<br>1/21/2019, 10:25:45 PM :: clnt007i - Student count: 61 from /pink/pink-floor-6ff9f54f44-zpchp<br>1/21/2019, 10:25:45 PM :: clnt010i - Instructor count: 1 from /pink/pink-floor-6ff9f54f44-<br><br>The clnt007i and clnt010i messages are produced once the file has been loacted.</code></pre><br><br><br><blockquote><p>Second method is to exec into the running container and create the file from a shell prompt.  This method requires &#39;sh&#39; capability must be installed in the container for this to work.</p></blockquote><br><br><pre><code>Command to get pods in namespace<br>    oc -n &lt;team&gt; get pods                 &lt;=== Replace &lt;team&gt;<br><br>Example output from &quot;pink&quot; namespace    <br>    NAME                              READY     STATUS    RESTARTS   AGE<br>    pink-floor-6ff9f54f44-zpchp       1/1       Running   0          41s<br><br>Open a terminal session with the running session<br><br><br>Add the team.txt file using the touch command from outside the container.<br>    oc exec -it -n pink pink-floor-6ff9f54f44-zpchp -- sh<br><br>    The above command is using &#39;sh&#39;.  The &#39;sh&#39; capability must be installed in the container for this to work.<br><br>Example result output:<br>    /app #<br><br><br>Create the file using touch by entering the following command:<br>    touch team.txt                   <br><br>Notice the &quot;/app&quot; directory is not included as part of the touch command since the prompt is open to that directory.<br><br><br>Example result output:  (wait a few seconds for the messages to show)<br><br>1/21/2019, 10:25:30 PM :: clnt014i - Create the file in the running container.<br>1/21/2019, 10:25:45 PM :: ----------------------------------------------------------------------------------<br>1/21/2019, 10:25:45 PM :: clnt008i - File located.  Reporting to collector.<br>1/21/2019, 10:25:45 PM :: ----------------------------------------------------------------------------------<br>1/21/2019, 10:25:45 PM :: clnt007i - Student count: 61 from /pink/pink-floor-6ff9f54f44-zpchp<br>1/21/2019, 10:25:45 PM :: clnt010i - Instructor count: 1 from /pink/pink-floor-6ff9f54f44-<br><br>The clnt007i and clnt010i messages are produced once the file has been loacted.</code></pre><br><br><br><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-6">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/gonzo.yaml" target="_blank">gonzo.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/gonzo_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-7">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/gonzo:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;/app/gonzo.sh&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-gonzo is failing creation.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the failure.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the gonzo.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deplloyed</td></tr></tbody></table><hr><ul><li>What ENTRYPOINT or CMD is defined for the Docker image?</li><li>What container &quot;command&quot; parameter is defined for the pod definition?</li><li>Command: docker history ibmicpcoc/gonzo --no-trunc can also be used to check the docker image.</li><li>The gonzo.yaml must be modified to correct the issue.  You will not be allowd to rebuild or modify the Docker image.</li></ul><hr><h4 id="diagnosis-7">Diagnosis</h4><br><pre><code>Command to get pods in namespace<br>    oc -n &lt;team&gt; get pods                 &lt;=== Replace &lt;team&gt;<br><br>Example output from &quot;pink&quot; namespace    <br>    NAME                              READY     STATUS             RESTARTS   AGE<br>    pink-gonzo-75d79787b7-88pnr       0/1       CrashLoopBackOff   4          2m<br><br><br>Command to describe pod that is failing.  Following example using above pod and pink namespace.<br>    oc describe pod pink-gonzo-75d79787b7-88pnr -n pink<br><br>Example output:<br><br>Name:               pink-gonzo-75d79787b7-88pnr<br>Namespace:          pink<br>Priority:           0<br>PriorityClassName:  &lt;none&gt;<br>Node:               10.186.56.85/10.186.56.85<br>Start Time:         Mon, 21 Jan 2019 18:13:15 -0600<br>Labels:             app=pink-gonzo<br>                    pod-template-hash=3183534363<br>. . .<br>        portions of output removed<br>. . .<br><br>Conditions:<br>  Type              Status<br>  Initialized       True<br>  Ready             False<br>  ContainersReady   False<br>  PodScheduled      True<br>Volumes:<br>  default-token-mq64m:<br>    Type:        Secret (a volume populated by a Secret)<br>    SecretName:  default-token-mq64m<br>    Optional:    false<br>QoS Class:       Burstable<br>Node-Selectors:  &lt;none&gt;<br>Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>                 node.kubernetes.io/not-ready:NoExecute for 300s<br>                 node.kubernetes.io/unreachable:NoExecute for 300s<br>Events:<br>  Type     Reason     Age                 From                   Message<br>  ----     ------     ----                ----                   -------<br>  Normal   Scheduled  11m                 default-scheduler      Successfully assigned pink/pink-gonzo-75d79787b7-88pnr to 10.186.56.85<br>  Normal   Created    10m (x4 over 11m)   kubelet, 10.186.56.85  Created container<br>  Normal   Started    10m (x4 over 11m)   kubelet, 10.186.56.85  Started container<br>  Normal   Pulling    9m (x5 over 11m)    kubelet, 10.186.56.85  pulling image &quot;ibmicpcoc/gonzo:latest&quot;<br>  Normal   Pulled     9m (x5 over 11m)    kubelet, 10.186.56.85  Successfully pulled image &quot;ibmicpcoc/gonzo:latest&quot;<br>  Warning  BackOff    58s (x46 over 11m)  kubelet, 10.186.56.85  Back-off restarting failed container                <br></code></pre><br><p>In the &quot;Events&quot; section review the &quot;Message&quot; from the entry with &quot;Type&quot; Warning and &quot;Reason&quot; BackOff<br><br></p><br><pre><code>... Back-off restarting failed container</code></pre><p>Check the image for the command or entrypoint defined to execute when the container is created</p><br><pre><code>Review the Dockerfile provided in the Resources section of this lab.<br><br>Browse the Dockerfile<br>    Click the Dockerfile link in resource section and review the entrypoint or command defined to start when container is created.<br><br>(or)    <br><br>Check the Docker image<br>    docker history ibmicpcoc/gonzo --no-trunc     </code></pre><h4 id="problem-discovered-7">Problem discovered</h4><p>The container is ending as soon as it starts.  The entrypoint or command that executes when the container starts is not defined in either the Dockerfile or gonzo.yaml file.  </p><h4 id="resolution-7">Resolution</h4><p>Add the &quot;command&quot; parameter to the pod container definition using the file gonzo.yaml provided in the Resources section of this lab.  The &quot;command&quot; parameter should start the bash script /app/gonzo.sh using /bin/bash</p><br><pre><code>command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;/app/gonzo.sh&quot;]</code></pre><pre><code>Add the &quot;command&quot; parameter to the container:apiVersion: apps/v1kind: Deploymentmetadata:  name: pink-gonzo  namespace: pink  labels:    app: pink-gonzospec:  selector:    matchLabels:      app: pink-gonzo  replicas: 1  template:    metadata:      labels:        app: pink-gonzo    spec:      containers:      - name: pink-gonzo        image: ibmicpcoc/gonzo:latest        imagePullPolicy: Always        command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;/app/gonzo.sh&quot;]    &lt;=== insert this line. . .  reaminder of file not shown . . .Save the modifed fileCommand to delete the current deployed pod    oc -n &lt;team&gt; delete -f gonzo.yamlExample output:    deployment.apps/pink-gonzo deleteCommand to deploy the updated pod    oc -n &lt;team&gt; create -f gonzo.yamlExample output:    deployment.apps/pink-gonzo createdCommand to verify the updated pod is running    oc -n &lt;team&gt; get podsExample output:    NAME                              READY     STATUS      RESTARTS   AGE    pink-gonzo-67834787b7-234xy       1/1       Running     0          2m</code></pre><br><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-7">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/igloo.yaml" target="_blank">igloo.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/igloo_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-8">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/igloo:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Misc</td><td align="left">Application waits</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-igloo is frequently restarting.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the pod to restart frequently.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to determine how long the http server waits to be started.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the igloo.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the NodePort for the red-igloo service.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the IP address for the master node.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Using the above NodePort and the master ip address access the url: http://<master ip>:<NodePort></td></tr></tbody></table><hr><ul><li>How long do both probes delay before starting?</li></ul><hr><h4 id="diagnosis-8">Diagnosis</h4><br><pre><code>Command to get pods in namespace<br>    oc -n &lt;team&gt; get pods                 &lt;=== Replace &lt;team&gt;<br><br>Example output from &quot;red&quot; namespace    <br>   NAME                         READY     STATUS    RESTARTS   AGE<br>   red-igloo-7b85976d87-x6z6r   0/1       Running   3          2m<br><br><br>Command to view the pod details<br>    oc describe po red-igloo-7b85976d87-x6z6r<br>    Name:               red-igloo-7b85976d87-x6z6r<br>    Namespace:          red<br>    Priority:           0<br>    PriorityClassName:  &lt;none&gt;<br>    Node:               gfstst.169.62.225.201.nip.io/169.62.225.201<br>    Start Time:         Tue, 03 Sep 2019 20:06:13 -0400<br>    Labels:             app=red-igloo<br>                        pod-template-hash=3641532843<br>    Annotations:        openshift.io/scc=restricted<br>    Status:             Running<br>    IP:                 10.129.0.94<br>    Controlled By:      ReplicaSet/red-igloo-7b85976d87<br>    Containers:<br>      red-igloo:<br>        Container ID:   docker://e9b6049395fa281c1ca0d6e63001ac3226fc211c5948bf1673023c9dc6f74f37<br>        Image:          ibmicpcoc/igloo:latest<br>        Image ID:       docker-pullable://docker.io/ibmicpcoc/igloo@sha256:4968f5c1ca641e3267d9a163c68eceb307973e06a30df51a47d86dcd0e301a40<br>        Port:           &lt;none&gt;<br>        Host Port:      &lt;none&gt;<br>        State:          Running<br>          Started:      Tue, 03 Sep 2019 20:06:49 -0400<br>        Last State:     Terminated<br>          Reason:       Error<br>          Exit Code:    137<br>          Started:      Tue, 03 Sep 2019 20:06:16 -0400<br>          Finished:     Tue, 03 Sep 2019 20:06:48 -0400<br>        Ready:          False<br>        Restart Count:  1<br>        Requests:<br>          cpu:      50m<br>          memory:   50Mi<br>        Liveness:   http-get http://:4100/health delay=1s timeout=1s period=2s #success=1 #failure=1<br>        Readiness:  http-get http://:4100/ready delay=1s timeout=1s period=5s #success=1 #failure=3<br>        Environment:<br>          APP_NAMESPACE:      red (v1:metadata.namespace)<br>          APP_NAME:           red-igloo-7b85976d87-x6z6r (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>        Mounts:<br>          /var/run/secrets/kubernetes.io/serviceaccount from default-token-dxnzt (ro)<br>    Conditions:<br>      Type              Status<br>      Initialized       True<br>      Ready             False<br>      ContainersReady   False<br>      PodScheduled      True<br>    Volumes:<br>      default-token-dxnzt:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  default-token-dxnzt<br>        Optional:    false<br>    QoS Class:       Burstable<br>    Node-Selectors:  node-role.kubernetes.io/compute=true<br>    Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>    Events:<br>      Type     Reason     Age               From                                   Message<br>      ----     ------     ----              ----                                   -------<br>      Normal   Scheduled  42s               default-scheduler                      Successfully assigned red/red-igloo-7b85976d87-x6z6r to gfstst.169.62.225.201.nip.io<br>      Normal   Pulling    7s (x2 over 40s)  kubelet, gfstst.169.62.225.201.nip.io  pulling image &quot;ibmicpcoc/igloo:latest&quot;<br>      Normal   Killing    7s                kubelet, gfstst.169.62.225.201.nip.io  Killing container with id docker://red-igloo:Container failed liveness probe.. Container will be killed and recreated.<br>      Normal   Pulled     6s (x2 over 39s)  kubelet, gfstst.169.62.225.201.nip.io  Successfully pulled image &quot;ibmicpcoc/igloo:latest&quot;<br>      Normal   Created    6s (x2 over 39s)  kubelet, gfstst.169.62.225.201.nip.io  Created container<br>      Normal   Started    6s (x2 over 39s)  kubelet, gfstst.169.62.225.201.nip.io  Started container<br>      Warning  Unhealthy  4s (x2 over 38s)  kubelet, gfstst.169.62.225.201.nip.io  Liveness probe failed: Get http://10.129.0.94:4100/health: dial tcp 10.129.0.94:4100: connect: connection refused<br>      Warning  Unhealthy  2s (x3 over 37s)  kubelet, gfstst.169.62.225.201.nip.io  Readiness probe failed: Get http://10.129.0.94:4100/ready: dial tcp 10.129.0.94:4100: connect: connection refused<br></code></pre><br><p>Command to view the pod logs<br>     oc logs red-igloo-5dd5b6c7b8-jqdvr</p><br><p>Example output<br>    9/4/2019, 1:54:54 AM :: iglo900i - Waiting 10 seconds to start HTTP server  </p><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-8">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/jazzy.yaml" target="_blank">jazzy.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/jazzy_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-9">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/jazzy:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">9000</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Misc</td><td align="left">Application waits</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-jazzy is frequently restarting.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the pod to restart frequently.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to determine how long the application http server waits to be started.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the igloo.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the NodePort for the red-igloo service.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the IP address for the master node.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Using the above NodePort and the master ip address access the url: http://<master ip>:<NodePort></td></tr></tbody></table><hr><ul><li>How long do Readiness and Liveness probes delay before starting?</li></ul><hr><h4 id="diagnosis-9">Diagnosis</h4><br><pre><code>Command to get pods in namespace<br>    oc -n &lt;team&gt; get pods                 &lt;=== Replace &lt;team&gt;<br><br>Example output from &quot;red&quot; namespace    <br>   NAME                         READY     STATUS    RESTARTS   AGE<br>   red-igloo-7b85976d87-x6z6r   0/1       Running   3          2m<br><br><br>Command to view the pod details<br>    oc describe po red-igloo-7b85976d87-x6z6r<br>    Name:               red-igloo-7b85976d87-x6z6r<br>    Namespace:          red<br>    Priority:           0<br>    PriorityClassName:  &lt;none&gt;<br>    Node:               gfstst.169.62.225.201.nip.io/169.62.225.201<br>    Start Time:         Tue, 03 Sep 2019 20:06:13 -0400<br>    Labels:             app=red-igloo<br>                        pod-template-hash=3641532843<br>    Annotations:        openshift.io/scc=restricted<br>    Status:             Running<br>    IP:                 10.129.0.94<br>    Controlled By:      ReplicaSet/red-igloo-7b85976d87<br>    Containers:<br>      red-igloo:<br>        Container ID:   docker://e9b6049395fa281c1ca0d6e63001ac3226fc211c5948bf1673023c9dc6f74f37<br>        Image:          ibmicpcoc/igloo:latest<br>        Image ID:       docker-pullable://docker.io/ibmicpcoc/igloo@sha256:4968f5c1ca641e3267d9a163c68eceb307973e06a30df51a47d86dcd0e301a40<br>        Port:           &lt;none&gt;<br>        Host Port:      &lt;none&gt;<br>        State:          Running<br>          Started:      Tue, 03 Sep 2019 20:06:49 -0400<br>        Last State:     Terminated<br>          Reason:       Error<br>          Exit Code:    137<br>          Started:      Tue, 03 Sep 2019 20:06:16 -0400<br>          Finished:     Tue, 03 Sep 2019 20:06:48 -0400<br>        Ready:          False<br>        Restart Count:  1<br>        Requests:<br>          cpu:      50m<br>          memory:   50Mi<br>        Liveness:   http-get http://:4100/health delay=1s timeout=1s period=2s #success=1 #failure=1<br>        Readiness:  http-get http://:4100/ready delay=1s timeout=1s period=5s #success=1 #failure=3<br>        Environment:<br>          APP_NAMESPACE:      red (v1:metadata.namespace)<br>          APP_NAME:           red-igloo-7b85976d87-x6z6r (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>        Mounts:<br>          /var/run/secrets/kubernetes.io/serviceaccount from default-token-dxnzt (ro)<br>    Conditions:<br>      Type              Status<br>      Initialized       True<br>      Ready             False<br>      ContainersReady   False<br>      PodScheduled      True<br>    Volumes:<br>      default-token-dxnzt:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  default-token-dxnzt<br>        Optional:    false<br>    QoS Class:       Burstable<br>    Node-Selectors:  node-role.kubernetes.io/compute=true<br>    Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>    Events:<br>      Type     Reason     Age               From                                   Message<br>      ----     ------     ----              ----                                   -------<br>      Normal   Scheduled  42s               default-scheduler                      Successfully assigned red/red-igloo-7b85976d87-x6z6r to gfstst.169.62.225.201.nip.io<br>      Normal   Pulling    7s (x2 over 40s)  kubelet, gfstst.169.62.225.201.nip.io  pulling image &quot;ibmicpcoc/igloo:latest&quot;<br>      Normal   Killing    7s                kubelet, gfstst.169.62.225.201.nip.io  Killing container with id docker://red-igloo:Container failed liveness probe.. Container will be killed and recreated.<br>      Normal   Pulled     6s (x2 over 39s)  kubelet, gfstst.169.62.225.201.nip.io  Successfully pulled image &quot;ibmicpcoc/igloo:latest&quot;<br>      Normal   Created    6s (x2 over 39s)  kubelet, gfstst.169.62.225.201.nip.io  Created container<br>      Normal   Started    6s (x2 over 39s)  kubelet, gfstst.169.62.225.201.nip.io  Started container<br>      Warning  Unhealthy  4s (x2 over 38s)  kubelet, gfstst.169.62.225.201.nip.io  Liveness probe failed: Get http://10.129.0.94:4100/health: dial tcp 10.129.0.94:4100: connect: connection refused<br>      Warning  Unhealthy  2s (x3 over 37s)  kubelet, gfstst.169.62.225.201.nip.io  Readiness probe failed: Get http://10.129.0.94:4100/ready: dial tcp 10.129.0.94:4100: connect: connection refused</code></pre><p>Checking the running pod for application information regarding the startup delay.   </p><br><pre><code>Command to view the pod logs   <br>     oc logs red-igloo-5dd5b6c7b8-jqdvr<br><br>Example output    <br>    9/4/2019, 1:54:54 AM :: iglo900i - Waiting 10 seconds to start HTTP server  </code></pre><h4 id="problem-discovered-8">Problem discovered</h4><p>The Readiness and Liveness probes do not delay long enough to allow the aplication to start.  </p><h4 id="resolution-8">Resolution</h4><p>Modify the ReadinessProbe initialDelaySeconds to be longer than the ten seconds the application takes to start.   Also modify the LivenessProbe initialDelaySeconds and periodSeconds to be longer than the ten seconds the application takes to start.</p><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-9">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/lacey.yaml" target="_blank">lacey.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/lacey_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-10">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/lacey:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Misc</td><td align="left">Application waits</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-igloo is frequently restarting.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the pod to restart frequently.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to determine how long the application http server waits to be started.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the igloo.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the NodePort for the red-igloo service.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the IP address for the master node.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Using the above NodePort and the master ip address access the url: http://<master ip>:<NodePort></td></tr></tbody></table><hr><p>Provide a hint.</p><hr><h4 id="diagnosis-10">Diagnosis</h4><p>Checking the running pod for application information.   </p><br><pre><code>Command to view the pod logs   <br>     oc logs red-igloo-5dd5b6c7b8-jqdvr<br><br>Example output    <br>    9/4/2019, 1:54:54 AM :: iglo900i - Waiting 10 seconds to start HTTP server  </code></pre><h4 id="problem-discovered-9">Problem discovered</h4><p>Describe the problem.</p><h4 id="resolution-9">Resolution</h4><p>Describe the resolution.</p><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-10">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/magma.yaml" target="_blank">magma.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/magma_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-11">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/magma:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr></tbody></table><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Secret Parameter</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Name</td><td align="left">&#60;team&#62;-secret-file</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Content</td><td align="left">Base64 encoded: debug me</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Type</td><td align="left">Opaque</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Mount</td><td align="left">/var/config</td></tr><tr style="background-color: #f8f8f8;"><td align="left">File</td><td align="left">secret.txt</td></tr></tbody></table><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">ConfigMap Parameter</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Name</td><td align="left">&#60;team&#62;-configmap-file</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Content</td><td align="left">debug</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Mount</td><td align="left">/var/secret</td></tr><tr style="background-color: #f8f8f8;"><td align="left">File</td><td align="left">config.txt</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-magma has a status of ContainerCreating.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the pod to be in this status.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Describe the pod to assist in determining why this issue is occurring.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to determine how long the application http server waits to be started.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the magma.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr></tbody></table><hr><p>Create the secret and configmap.</p><hr><h4 id="diagnosis-11">Diagnosis</h4><p>Checking the running pod for information.   </p><br><pre><code>Command to view pod status<br>    oc get pods -n &lt;team&gt;<br><br>Example output<br>    NAME                         READY     STATUS              RESTARTS   AGE<br>    red-magma-6c4b56dbc9-kdtkv   0/1       ContainerCreating   0          11s<br><br><br>Command to describe the pod   <br>     oc describe po red-magma-6c4b56dbc9-kdtkv<br><br>Example output    <br>    Name:               red-magma-6c4b56dbc9-kdtkv<br>    Namespace:          red<br>    Priority:           0<br>    PriorityClassName:  &lt;none&gt;<br>    Node:               gfstst.169.62.225.207.nip.io/169.62.225.207<br>    Start Time:         Sat, 07 Sep 2019 12:48:53 -0400<br>    Labels:             app=red-magma<br>                        pod-template-hash=2706128675<br>    Annotations:        openshift.io/scc=restricted<br>    Status:             Pending<br>    IP:<br>    Controlled By:      ReplicaSet/red-magma-6c4b56dbc9<br>    Containers:<br>      red-magma:<br>        Container ID:<br>        Image:         ibmicpcoc/magma:latest<br>        Image ID:<br>        Port:          &lt;none&gt;<br>        Host Port:     &lt;none&gt;<br>        Command:<br>          node<br>          app.js<br>        State:          Waiting<br>          Reason:       ContainerCreating<br>        Ready:          False<br>        Restart Count:  0<br>        Requests:<br>          cpu:     50m<br>          memory:  50Mi<br>        Environment:<br>          APP_NAMESPACE:      red (v1:metadata.namespace)<br>          APP_NAME:           red-magma-6c4b56dbc9-kdtkv (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>        Mounts:<br>          /var/config from configvol (rw)<br>          /var/run/secrets/kubernetes.io/serviceaccount from default-token-dxnzt (ro)<br>          /var/secret from secretvol (rw)<br>    Conditions:<br>      Type              Status<br>      Initialized       True<br>      Ready             False<br>      ContainersReady   False<br>      PodScheduled      True<br>    Volumes:<br>      configvol:<br>        Type:      ConfigMap (a volume populated by a ConfigMap)<br>        Name:      red-configmap-file<br>        Optional:  false<br>      secretvol:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  red-secret-file<br>        Optional:    false<br>      default-token-dxnzt:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  default-token-dxnzt<br>        Optional:    false<br>    QoS Class:       Burstable<br>    Node-Selectors:  node-role.kubernetes.io/compute=true<br>    Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>    Events:<br>      Type     Reason       Age                From                                   Message<br>      ----     ------       ----               ----                                   -------<br>      Normal   Scheduled    42s                default-scheduler                      Successfully assigned red/red-magma-6c4b56dbc9-kdtkv to gfstst.169.62.225.207.nip.io<br>      Warning  FailedMount  10s (x7 over 42s)  kubelet, gfstst.169.62.225.207.nip.io  MountVolume.SetUp failed for volume &quot;configvol&quot; : configmaps &quot;red-configmap-file&quot; not found<br>      Warning  FailedMount  10s (x7 over 42s)  kubelet, gfstst.169.62.225.207.nip.io  MountVolume.SetUp failed for volume &quot;secretvol&quot; : secrets &quot;red-secret-file&quot; not found<br><br></code></pre><br><h4 id="problem-discovered-10">Problem discovered</h4><p>Two volume mounts are failing for configvol and secretvol volumes.  These mounts require a configmap and secret definitons that are not found.</p><h4 id="resolution-10">Resolution</h4><p>Create a secret of opaque type with base64 encoded value &#39;debug me&#39; without the quotes.  This secret is accessed via a volume</p><p>oc create secret generic apikey --from-file=./apikey.txtsecret &quot;apikey&quot; created </p><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="desired-environment">Desired environment:</h4><p>Deployment of an application that uses persistent storage.  The storage is implemented as static storage with a PV and PVC.  The PV uses NFS based storage. </p><p>Note: <strong>This lab requires the student to resolve multiple issues</strong></p><h4 id="resources-11">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/offer.yaml" target="_blank">offer.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/offer_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-12">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/offer:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr></tbody></table><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">PV Parm</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">metadata.name</td><td align="left">&#60;team&#62;-pv</td></tr><tr style="background-color: #f8f8f8;"><td align="left">metadata.labels.user</td><td align="left">&#60;team&#62;</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.capacity.storage</td><td align="left">1Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.accessModes</td><td align="left">ReadWriteOnce</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.nfs.path</td><td align="left">/storage/&#60;team&#62;/pvc001</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.nfs.server</td><td align="left"><student ssh ip></td></tr><tr style="background-color: #f8f8f8;"><td align="left">persistentVolumeReclaimPolicy</td><td align="left">recycle</td></tr></tbody></table><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">PVC Parm</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">metadata.name</td><td align="left">&#60;team&#62;-offer</td></tr><tr style="background-color: #f8f8f8;"><td align="left">metadata.namespace</td><td align="left">&#60;team&#62;</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.resources.requests.storage</td><td align="left">1Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.accessModes</td><td align="left">ReadWriteOnce</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.selector.matchLabels.user</td><td align="left">&#60;team&#62;</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">Research multiple isses that are preventing the succesful deployment of the pod.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">During the debugging be sure to describe the pod and view the pod logs.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the offer.yaml file to correct the issues. (repeat)</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr></tbody></table><hr><p>You have admin rights to create directories.</p><hr><h4 id="diagnosis-1-1">Diagnosis 1</h4><p>Checking the running pod for application information.   </p><br><pre><code>Command to describe the pod   <br>     oc describe po red-offer-6cdf4749df-rtfwg<br><br>Example output    <br><br>    Name:               red-offer-6cdf4749df-rtfwg<br>    Namespace:          red<br>    Priority:           0<br>    PriorityClassName:  &lt;none&gt;<br>    Node:               &lt;none&gt;<br>    Labels:             app=red-offer<br>                        pod-template-hash=2789030589<br>    Annotations:        openshift.io/scc=restricted<br>    Status:             Pending<br>    IP:<br>    Controlled By:      ReplicaSet/red-offer-6cdf4749df<br>    Containers:<br>      red-offer:<br>        Image:      ibmicpcoc/offer:latest<br>        Port:       &lt;none&gt;<br>        Host Port:  &lt;none&gt;<br>        Command:<br>          node<br>          app.js<br>        Requests:<br>          cpu:     50m<br>          memory:  50Mi<br>        Environment:<br>          APP_NAMESPACE:      red (v1:metadata.namespace)<br>          APP_NAME:           red-offer-6cdf4749df-rtfwg (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>        Mounts:<br>          /data from offer-data (rw)<br>          /var/run/secrets/kubernetes.io/serviceaccount from default-token-dxnzt (ro)<br>    Conditions:<br>      Type           Status<br>      PodScheduled   False<br>    Volumes:<br>      offer-data:<br>        Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)<br>        ClaimName:  red-offr<br>        ReadOnly:   false<br>      default-token-dxnzt:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  default-token-dxnzt<br>        Optional:    false<br>    QoS Class:       Burstable<br>    Node-Selectors:  node-role.kubernetes.io/compute=true<br>    Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>    Events:<br>      Type     Reason            Age               From               Message<br>      ----     ------            ----              ----               -------<br>      Warning  FailedScheduling  1m (x25 over 2m)  default-scheduler  persistentvolumeclaim &quot;red-offr&quot; not found    <br><br></code></pre><br><h4 id="problem-1-discovered">Problem 1 discovered</h4><p>Message from the describe indicates the PVC is not found.  The Deployment spec.template.spec.volumes.persistentVolumeClaim.claimName does not match the name of the defined PVC. </p><h4 id="resolution-1-1">Resolution 1</h4><p>Change the spec.template.spec.volumes.persistentVolumeClaim.claimName parameter to match the name of the defined PVC: &#60;team&#62;-offer (notice the dash between &#60;team&#62; and offer)</p><h4 id="diagnosis-2-1">Diagnosis 2</h4><p>Checking the running pod for application information.   </p><br><pre><code>Command to view the pod logs   <br>     oc describe po red-offer-66c6488b56-m9wfr<br><br><br>Example output    <br><br>    Name:               red-offer-66c6488b56-m9wfr<br>    Namespace:          red<br>    Priority:           0<br>    PriorityClassName:  &lt;none&gt;<br>    Node:               gfstst.169.62.225.201.nip.io/169.62.225.201<br>    Start Time:         Sun, 08 Sep 2019 12:54:12 -0400<br>    Labels:             app=red-offer<br>                        pod-template-hash=2272044612<br>    Annotations:        openshift.io/scc=restricted<br>    Status:             Pending<br>    IP:<br>    Controlled By:      ReplicaSet/red-offer-66c6488b56<br>    Containers:<br>      red-offer:<br>        Container ID:<br>        Image:         ibmicpcoc/offer:latest<br>        Image ID:<br>        Port:          &lt;none&gt;<br>        Host Port:     &lt;none&gt;<br>        Command:<br>          node<br>          app.js<br>        State:          Waiting<br>          Reason:       ContainerCreating<br>        Ready:          False<br>        Restart Count:  0<br>        Requests:<br>          cpu:     50m<br>          memory:  50Mi<br>        Environment:<br>          APP_NAMESPACE:      red (v1:metadata.namespace)<br>          APP_NAME:           red-offer-66c6488b56-m9wfr (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>        Mounts:<br>          /data from offer-data (rw)<br>          /var/run/secrets/kubernetes.io/serviceaccount from default-token-dxnzt (ro)<br>    Conditions:<br>      Type              Status<br>      Initialized       True<br>      Ready             False<br>      ContainersReady   False<br>      PodScheduled      True<br>    Volumes:<br>      offer-data:<br>        Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)<br>        ClaimName:  red-offer<br>        ReadOnly:   false<br>      default-token-dxnzt:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  default-token-dxnzt<br>        Optional:    false<br>    QoS Class:       Burstable<br>    Node-Selectors:  node-role.kubernetes.io/compute=true<br>    Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>    Events:<br>      Type     Reason       Age   From                                   Message<br>      ----     ------       ----  ----                                   -------<br>      Normal   Scheduled    17s   default-scheduler                      Successfully assigned red/red-offer-66c6488b56-m9wfr to gfstst.169.62.225.201.nip.io<br>      Warning  FailedMount  16s   kubelet, gfstst.169.62.225.201.nip.io  MountVolume.SetUp failed for volume &quot;red-pv&quot; : mount failed: exit status 32<br>    Mounting command: systemd-run<br>    Mounting arguments: --description=Kubernetes transient mount for /var/lib/origin/openshift.local.volumes/pods/48d45636-d259-11e9-8c57-0607f5770d4d/volumes/kubernetes.io~nfs/red-pv --scope -- mount -t nfs 169.62.225.199:/storage/red/pvc001 /var/lib/origin/openshift.local.volumes/pods/48d45636-d259-11e9-8c57-0607f5770d4d/volumes/kubernetes.io~nfs/red-pv<br>    Output: Running scope as unit run-126180.scope.<br>    mount.nfs: mounting 169.62.225.199:/storage/red/pvc001 failed, reason given by server: No such file or directory<br>      Warning  FailedMount  16s  kubelet, gfstst.169.62.225.201.nip.io  MountVolume.SetUp failed for volume &quot;red-pv&quot; : mount failed: exit status 32<br><br></code></pre><br><h4 id="problem-2-discovered">Problem 2 discovered</h4><p>Message from the describe indicates the PV mount failed.  This is caused because the path does not exist.  </p><h4 id="resolution-2-1">Resolution 2</h4><p>Two options exist to correct this issue:</p><p><strong>Option 1</strong><br>Change the the PV nfs path to a path that exists.</p><p><strong>Option 2</strong>Create the path on the NFS server.   </p><p>Delete and redeploy all resources i.e. PV, PVC, Pod, etc. </p><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="desired-environment-1">Desired environment</h4><p>Deployment of an application that uses persistent storage.  The storage is implemented as dynamic storage. </p><p>Note: <strong>This lab requires the student to resolve multiple issues</strong></p><h4 id="resources-12">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/panda.yaml" target="_blank">panda.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/panda_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-13">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/panda:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A statefulset that begins with &#60;team&#62;-panda is failing.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the statefulset to fail.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to determine how long the application http server waits to be started.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the panda.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Validate if the statefulset deployed.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the pod deployed. If not research why not.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the panda.yaml file to correct any issues.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Validate if the statefulset and pod deployed.</td></tr></tbody></table><hr><p>Did resource type is created in the yaml?<br>Ensure to review and diagnois all resource types.</p><hr><h4 id="diagnosis-1-2">Diagnosis 1</h4><p>Checking the pod information.   </p><br><pre><code>Command to describe the statefulset  <br>     oc describe statefulset red-panda<br><br>Example output    <br><br>    Name:               red-panda<br>    Namespace:          red<br>    CreationTimestamp:  Sun, 08 Sep 2019 14:17:00 -0400<br>    Selector:           app=red-panda<br>    Labels:             app=red-panda<br>    Annotations:        &lt;none&gt;<br>    Replicas:           1 desired | 0 total<br>    Update Strategy:    RollingUpdate<br>    Pods Status:        0 Running / 0 Waiting / 0 Succeeded / 0 Failed<br>    Pod Template:<br>      Labels:  app=red-panda<br>      Containers:<br>       red-panda:<br>        Image:      ibmicpcoc/offer:latest<br>        Port:       &lt;none&gt;<br>        Host Port:  &lt;none&gt;<br>        Command:<br>          node<br>          app.js<br>        Requests:<br>          cpu:     50m<br>          memory:  50Mi<br>        Environment:<br>          APP_NAMESPACE:       (v1:metadata.namespace)<br>          APP_NAME:            (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>        Mounts:<br>          /data from panda-d (rw)<br>      Volumes:  &lt;none&gt;<br>    Volume Claims:<br>      Name:          panda-data<br>      StorageClass:  rdb<br>      Labels:        &lt;none&gt;<br>      Annotations:   &lt;none&gt;<br>      Capacity:      1Mi<br>      Access Modes:  [ReadWriteOnce]<br>    Events:<br>      Type     Reason        Age                From                    Message<br>      ----     ------        ----               ----                    -------<br>      Warning  FailedCreate  24s (x16 over 1m)  statefulset-controller  create Pod red-panda-0 in StatefulSet red-panda failed error: Pod &quot;red-panda-0&quot; is invalid: spec.containers[0].volumeMounts[0].name: Not found: &quot;panda-d&quot;    <br></code></pre><br><h4 id="problem-1-discovered-1">Problem 1 discovered</h4><p>The panda-d volumeMount does not exist.</p><h4 id="resolution-1-2">Resolution 1</h4><p>Ensure the parameters volumeClaimTemplate.metadata.name match the stateful set spec.template.spec.container.volumeMounts.name</p><hr><h4 id="diagnosis-2-2">Diagnosis 2</h4><p>Checking the pod information.   </p><br><pre><code>Command to describe the statefulset  <br>     oc describe statefulset red-panda<br><br>Example output    <br>    Name:               red-panda-0<br>    Namespace:          red<br>    Priority:           0<br>    PriorityClassName:  &lt;none&gt;<br>    Node:               &lt;none&gt;<br>    Labels:             app=red-panda<br>                        controller-revision-hash=red-panda-89c55dc87<br>                        statefulset.kubernetes.io/pod-name=red-panda-0<br>    Annotations:        openshift.io/scc=restricted<br>    Status:             Pending<br>    IP:<br>    Controlled By:      StatefulSet/red-panda<br>    Containers:<br>      red-panda:<br>        Image:      ibmicpcoc/offer:latest<br>        Port:       &lt;none&gt;<br>        Host Port:  &lt;none&gt;<br>        Command:<br>          node<br>          app.js<br>        Requests:<br>          cpu:     50m<br>          memory:  50Mi<br>        Environment:<br>          APP_NAMESPACE:      red (v1:metadata.namespace)<br>          APP_NAME:           red-panda-0 (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>        Mounts:<br>          /data from panda-data (rw)<br>          /var/run/secrets/kubernetes.io/serviceaccount from default-token-dxnzt (ro)<br>    Conditions:<br>      Type           Status<br>      PodScheduled   False<br>    Volumes:<br>      panda-data:<br>        Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)<br>        ClaimName:  panda-data-red-panda-0<br>        ReadOnly:   false<br>      default-token-dxnzt:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  default-token-dxnzt<br>        Optional:    false<br>    QoS Class:       Burstable<br>    Node-Selectors:  node-role.kubernetes.io/compute=true<br>    Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>    Events:<br>      Type     Reason            Age                From               Message<br>      ----     ------            ----               ----               -------<br>      Warning  FailedScheduling  52s (x25 over 1m)  default-scheduler  pod has unbound PersistentVolumeClaims (repeated 3 times)<br><br><br>---<br><br>Command to get PVCs<br>    oc get persistentvolumeclaims<br><br>Example output:<br><br>    NAME                         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE<br>    panda-data-red-panda-0       Pending                                                                        rdb                   2h<br>    red-panda-data-red-panda-0   Bound     pvc-9d13c3eb-d263-11e9-8c57-0607f5770d4d   1Mi        RWO            managed-nfs-storage   2h<br><br>---<br><br>Command to describe the PVC<br>    oc describe pvc panda-data-red-panda-0<br><br>Example output:<br><br>    Name:          panda-data-red-panda-0<br>    Namespace:     red<br>    StorageClass:  rdb<br>    Status:        Pending<br>    Volume:<br>    Labels:        app=red-panda<br>    Annotations:   &lt;none&gt;<br>    Finalizers:    [kubernetes.io/pvc-protection]<br>    Capacity:<br>    Access Modes:<br>    Events:<br>      Type     Reason              Age                From                         Message<br>      ----     ------              ----               ----                         -------<br>      Warning  ProvisioningFailed  2m (x664 over 2h)  persistentvolume-controller  storageclass.storage.k8s.io &quot;rdb&quot; not found<br><br></code></pre><br><h4 id="problem-2-discovered-1">Problem 2 discovered</h4><p>The pod has a unbound PCV.  Getting the existing PVCs shows there is a Pending status.  Describe the status of the pending PVC. The describe output shows the storage class &#39;rdb&#39; does not exist.  </p><h4 id="resolution-2-2">Resolution 2</h4><p>Determine the available storage classess and redefine the storage definition using a valid storage class.</p><hr><br><pre><code><br>Command to determine the available storage classes  <br>     oc get storageclass<br><br>Example output:<br>    NAME                      PROVISIONER                AGE<br>    glusterfs-storage         kubernetes.io/glusterfs    10d<br>    glusterfs-storage-block   gluster.org/glusterblock   10d<br>    managed-nfs-storage       myokd/nfs                  10d </code></pre><br><hr><br><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="desired-environment-2">Desired environment</h4><p>Deploy a pod that is accessable external to the cluster via a route.  The desired route name is defined as a environment varialble.  This environment variable does not create the route but defines what route must be defined.</p><h4 id="resources-13">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/quake.yaml" target="_blank">quake.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/quake_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-14">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/quake:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr></tbody></table><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Route Parameter</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">metadata.lables.app</td><td align="left">&#60;team&#62;-quake</td></tr><tr style="background-color: #f8f8f8;"><td align="left">metadata.name</td><td align="left">hot-dog</td></tr><tr style="background-color: #f8f8f8;"><td align="left">metadata.namespace</td><td align="left">&#60;team&#62;</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.host</td><td align="left">must be determined by student</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.port.targetPort</td><td align="left">&#60;team&#62;-quake</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.to.kind</td><td align="left">Service</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.to.name</td><td align="left">&#60;team&#62;-quake</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.to.weight</td><td align="left">100</td></tr><tr style="background-color: #f8f8f8;"><td align="left">spec.wildcardPolicy</td><td align="left">None</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-quake is Back-off restarting.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the pod to restart frequently.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to aid in determining what is causing the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the quake.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr></tbody></table><hr><p>Define the missing route.</p><hr><h4 id="diagnosis-12">Diagnosis</h4><p>Checking the running pod for application information.   </p><br><pre><code>Command to view pods  <br>     oc get pods<br><br>Example output    <br>    NAME                        READY     STATUS    RESTARTS   AGE<br>    red-quake-d5f9cb9bb-fmw75   1/1       Running   0          4s<br><br>Command to describe pods  <br>     oc describe po red-quake-d5f9cb9bb-fmw75<br><br>Example output    <br>    Name:               red-quake-d5f9cb9bb-fmw75<br>    Namespace:          red<br>    Priority:           0<br>    PriorityClassName:  &lt;none&gt;<br>    Node:               gfstst.169.62.225.207.nip.io/169.62.225.207<br>    Start Time:         Sun, 08 Sep 2019 17:16:57 -0400<br>    Labels:             app=red-quake<br>                        pod-template-hash=819576566<br>    Annotations:        openshift.io/scc=restricted<br>    Status:             Running<br>    IP:                 10.130.0.228<br>    Controlled By:      ReplicaSet/red-quake-d5f9cb9bb<br>    Containers:<br>      red-quake:<br>        Container ID:   docker://00f351acce5c580fefba540e76291e68e2adaf76b8a7d503ed2cc1b5ff41124f<br>        Image:          ibmicpcoc/quake:v2<br>        Image ID:       docker-pullable://docker.io/ibmicpcoc/quake@sha256:4412f897746e13d7941ca6ba4a2e5a15769de47e5c7970dcc73adb3efc608545<br>        Port:           4100/TCP<br>        Host Port:      0/TCP<br>        State:          Terminated<br>          Reason:       Error<br>          Exit Code:    1<br>          Started:      Sun, 08 Sep 2019 17:17:03 -0400<br>          Finished:     Sun, 08 Sep 2019 17:17:04 -0400<br>        Last State:     Terminated<br>          Reason:       Error<br>          Exit Code:    1<br>          Started:      Sun, 08 Sep 2019 17:17:00 -0400<br>          Finished:     Sun, 08 Sep 2019 17:17:01 -0400<br>        Ready:          False<br>        Restart Count:  1<br>        Requests:<br>          cpu:     50m<br>          memory:  50Mi<br>        Environment:<br>          APP_NAMESPACE:      red (v1:metadata.namespace)<br>          APP_NAME:           red-quake-d5f9cb9bb-fmw75 (v1:metadata.name)<br>          COLLECTOR_CONFIG:   &lt;set to the key &#39;COLLECTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;   Optional: false<br>          INSTRUCTOR_CONFIG:  &lt;set to the key &#39;INSTRUCTOR_CONFIG&#39; of config map &#39;red-collector-config&#39;&gt;  Optional: false<br>          ROUTE:              hotdog-red.gfstst.169.62.225.197.nip.io<br>        Mounts:<br>          /var/run/secrets/kubernetes.io/serviceaccount from default-token-dxnzt (ro)<br>    Conditions:<br>      Type              Status<br>      Initialized       True<br>      Ready             False<br>      ContainersReady   False<br>      PodScheduled      True<br>    Volumes:<br>      default-token-dxnzt:<br>        Type:        Secret (a volume populated by a Secret)<br>        SecretName:  default-token-dxnzt<br>        Optional:    false<br>    QoS Class:       Burstable<br>    Node-Selectors:  node-role.kubernetes.io/compute=true<br>    Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule<br>    Events:<br>      Type     Reason     Age                From                                   Message<br>      ----     ------     ----               ----                                   -------<br>      Normal   Scheduled  18s                default-scheduler                      Successfully assigned red/red-quake-d5f9cb9bb-fmw75 to gfstst.169.62.225.207.nip.io<br>      Normal   Pulling    13s (x2 over 16s)  kubelet, gfstst.169.62.225.207.nip.io  pulling image &quot;ibmicpcoc/quake:v2&quot;<br>      Normal   Pulled     12s (x2 over 16s)  kubelet, gfstst.169.62.225.207.nip.io  Successfully pulled image &quot;ibmicpcoc/quake:v2&quot;<br>      Normal   Created    12s (x2 over 16s)  kubelet, gfstst.169.62.225.207.nip.io  Created container<br>      Normal   Started    12s (x2 over 15s)  kubelet, gfstst.169.62.225.207.nip.io  Started container<br>      Warning  BackOff    10s                kubelet, gfstst.169.62.225.207.nip.io  Back-off restarting failed container<br><br><br><br>Command to view logs of pod<br>    oc logs red-quake-d5f9cb9bb-fmw75<br><br>Example output:<br>    9/8/2019, 9:22:52 PM :: quak001i - Application random key: 63657248-92fd-434e-b31c-f610b279f8f8<br>    9/8/2019, 9:22:52 PM :: quak003i - Environment APP_NAMESPACE: red<br>    9/8/2019, 9:22:52 PM :: quak004i - Environment APP_NAME: Using random key = red-quake-d5f9cb9bb-fmw75<br>    9/8/2019, 9:22:52 PM :: quak013i - Environment COLLECTOR_CONFIG: http://red-student-ui<br>    9/8/2019, 9:22:52 PM :: quak014i - Environment INSTRUCTOR_CONFIG: http://dashboard.default<br>    9/8/2019, 9:22:52 PM :: quak014i - Environment ROUTE: hotdog-red.gfstst.169.62.225.197.nip.io<br>    9/8/2019, 9:22:52 PM :: jazz007i - Quake Server started, port: 4400<br>    9/8/2019, 9:22:52 PM :: quak011i - Initial request to route<br>    9/8/2019, 9:22:52 PM :: quak012e - Error getting to Route: http://hotdog-red.gfstst.169.62.225.197.nip.io message: null<br><br><br><br></code></pre><br><h4 id="problem-discovered-11">Problem discovered</h4><p>The pod logs show error message labled with id quak012e.  The route for the pod is not defined.</p><h4 id="resolution-11">Resolution</h4><p>Define the pod route with the provided information.</p><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-14">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/rainey.yaml" target="_blank">rainey.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/rainey_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-15">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/rainey:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Misc</td><td align="left">Application waits</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-igloo is frequently restarting.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the pod to restart frequently.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to determine how long the application http server waits to be started.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the igloo.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the NodePort for the red-igloo service.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the IP address for the master node.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Using the above NodePort and the master ip address access the url: http://<master ip>:<NodePort></td></tr></tbody></table><hr><p>Provide a hint.</p><hr><h4 id="diagnosis-13">Diagnosis</h4><p>Checking the running pod for application information.   </p><br><pre><code>Command to view the pod logs   <br>     oc logs red-igloo-5dd5b6c7b8-jqdvr<br><br>Example output    <br>    9/4/2019, 1:54:54 AM :: iglo900i - Waiting 10 seconds to start HTTP server  </code></pre><h4 id="problem-discovered-12">Problem discovered</h4><p>Describe the problem.</p><h4 id="resolution-12">Resolution</h4><p>Describe the resolution.</p><hr><hr><p>All references to &quot;team&quot; or &#60;team&#62; should be replaced with your team name which is the same as your namespace.</p><h4 id="resources-15">Resources</h4><ul><li>K8 yaml    - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/salty.yaml" target="_blank">salty.yaml</a></li><li>Dockerfile - <a href="https://github.com/IBM-ICP-CoC/fs-course/blob/master/usefulFiles/salty_Dockerfile" target="_blank">Dockerfile</a></li></ul><h4 id="useful-information-16">Useful information</h4><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Item</th><th align="left">Value</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">cpu:</td><td align="left">50m</td></tr><tr style="background-color: #f8f8f8;"><td align="left">memory:</td><td align="left">50Mi</td></tr><tr style="background-color: #f8f8f8;"><td align="left">image:</td><td align="left">ibmicpcoc/salty:latest</td></tr><tr style="background-color: #f8f8f8;"><td align="left">ports</td><td align="left">none</td></tr><tr style="background-color: #f8f8f8;"><td align="left">YAML</td><td align="left">command: [&quot;node&quot;, &quot;app.js&quot;]</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Misc</td><td align="left">Application waits</td></tr></tbody></table><hr><br><table class="table-bordered" cellspacing="10"><thead style="background-color: #eee;"><tr style="background-color: #f8f8f8;"><th align="left">Task description</th></tr></thead><tbody><tr style="background-color: #f8f8f8;"><td align="left">A pod that begins with &#60;team&#62;-igloo is frequently restarting.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Research the issue to determine what is causing the pod to restart frequently.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Reiview the pod log to determine how long the application http server waits to be started.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Edit the igloo.yaml file to correct the issue.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Verify the deployment successfully deployed.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the NodePort for the red-igloo service.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Get the IP address for the master node.</td></tr><tr style="background-color: #f8f8f8;"><td align="left">Using the above NodePort and the master ip address access the url: http://<master ip>:<NodePort></td></tr></tbody></table><hr><p>Provide a hint.</p><hr><h4 id="diagnosis-14">Diagnosis</h4><p>Checking the running pod for application information.   </p><br><pre><code>Command to view the pod logs   <br>     oc logs red-igloo-5dd5b6c7b8-jqdvr<br><br>Example output    <br>    9/4/2019, 1:54:54 AM :: iglo900i - Waiting 10 seconds to start HTTP server  </code></pre><h4 id="problem-discovered-13">Problem discovered</h4><p>Describe the problem.</p><h4 id="resolution-13">Resolution</h4><p>Describe the resolution.</p><hr><hr>